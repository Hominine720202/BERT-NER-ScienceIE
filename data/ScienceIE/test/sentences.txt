A few studies within the physiological domain are of special relevance to this work.
These include a performance analysis of a blood-flow LB solver using a range of sparse and non-sparse geometries [21] and a performance prediction model for lattice-Boltzmann solvers [22,23].
This performance prediction model can be applied largely to our HemeLB application, although HemeLB uses a different decomposition technique and performs real-time rendering and visualisation tasks during the LB simulations.
Mazzeo and Coveney [1] studied the scalability of an earlier version of HemeLB.
However, the current performance characteristics of HemeLB are substantially enhanced due to numerous subsequent advances in the code, amongst others: an improved hierarchical, compressed file format; the use of ParMETIS to ensure good load-balance; the coalesced communication patterns to reduce the overhead of rendering; use of compile-time polymorphism to avoid virtual function calls in inner loops.
An obvious metric to measure the monitoring performance between the different conditions would be to compare how many clicks the users made in average for each condition.
Furthermore of interest are the buffer values of the respective buffers at the time of the user's interaction with the simulation (e.g., the input buffer of a certain machine at the time of refilling it).
A relatively high average buffer value can e.g.
signify that the users do not trust that the respective mode of process monitoring conveys the need for interaction in time, leading the users to switching their attention to the process simulation in regular intervals, and performing interactions just in case.
A low average buffer can, on the other hand, signify that the users rely on the respective conditions’ ability to signal interaction needs.
On the other hand, if e.g.
an input buffer had already been completely depleted at the time of intervention, this may signify that the respective condition has failed to inform the users in time.
In many cases, participants used double clicks for their interactions, while a single click would have been sufficient, a fact that was perhaps not communicated clearly enough to the participants.
Therefore, if several clicks were performed directly one after another, only the first click was taken into account.
Similar numerical oscillations to those described above also emerge in the ISPM when utilising classical IBM kernels due to their lack of regularity (with discontinuous second derivatives).
Furthermore, it is important to remark that the immersed structure stresses are captured in the Lagrangian description and hence, in order to compute them accurately, it is important to ensure that these spurious oscillations are not introduced via the kernel interpolation functions.
In this paper, the authors have specifically designed a new family of kernel functions which do not introduce these spurious oscillations.
The kernel functions are obtained by taking into account discrete reproducibility conditions as originally introduced by Peskin [14] (in our case, tailor-made for Cartesian staggered grids) and regularity requirements to prevent the appearance of spurious oscillations when computing derivatives.
A Maple computer program has been developed to obtain explicit expressions for the new kernels.
PV cells are one of the most promising technologies for conversion of incident solar radiation into electric power.
However, this technology is still far from being able to compete with fossil fuel-based energy conversion technologies because of its relatively low efficiency and energy density.
Theoretically, there are three unavoidable losses that limit the solar conversion efficiency of a device with a single absorption threshold or band gap Eg: (1) incomplete absorption, where photons with energies below Eg are not absorbed; (2) thermalization or carrier cooling, where solar photons with sufficient energy generate electron-hole pairs and then immediately lose almost all energy in excess of Eg in the form of heat; and (3) radiative recombination, where a small fraction of the excited states radioactively recombine with the ground state at the maximum power output (Hanna & Nozik, 2006; Henry, 1980).
Taking an air mass of 1.5 as an example, for different band gap Eg these three losses can be calculated and the results are indicated by areas S1, S2, and S3 in Fig.
1.
Note that the area under the outer curve is the solar power per unit area, and that only S4 can be delivered to the load.
Although mean-field models have been used in all these settings, little analysis has been done on their behaviour as spatially extended dynamical systems.
In part, this is due to their staggering complexity.
The Liley model [15] considered here, for instance, consists of fourteen coupled Partial Differential Equations (PDEs) with strong nonlinearities, imposed by coupling between the mean membrane potentials and the mean synaptic inputs.
The model can be reduced to a system of Ordinary Differential Equations (ODEs) by considering only spatially homogeneous solutions, and the resulting system has been examined in detail using numerical bifurcation analysis (see [16] and references therein).
In order to compute equilibria, periodic orbits and such objects for the PDE model, we need a flexible, stable simulation code for the model and its linearization that can run in parallel to scale up to a domain size of about 2500cm2, the size of a full-grown human cortex.
We also need efficient, iterative solvers for linear problems with large, sparse matrices.
In this paper, we will show that all this can be accomplished in the open-source software package PETSc [17].
Our implementation consists of a number of functions in C that are available publicly [18].
Using measured data from two arable sites in the UK we have shown that lags can have significant impact on model evaluation and can affect both the level of correlation between measured and simulated data and the magnitude of the sums of the residuals.
Also, we used the division of MSE to three constituent statistics (SB, SDSD and LCS) to show how the level of correlation can affect the sum of residuals.
By dividing the algorithm-predicted series of lag values into monthly sets and examining the frequency distribution of the lags, certain patterns in these temporally patchy series have been identified.
A challenging task in relation to time lags between observed and simulated daily data, is to determine their cause.
This task becomes more difficult for model outputs such as soil N2O emissions that are driven by various interacting variables.
Even more so, because the measured N2O datasets and the measured datasets of their drivers (e.g.
soil moisture, soil N content) cover small time periods, they are not continuous and can vary widely in size.
In this study we implemented the algorithm using measured and simulated data for soil moisture (first and second example) and soil mineral N (second example), and compared its results with the respective results for N2O.
In our first example, we showed that the estimated lags in N2O prediction are related to the lags in soil moisture prediction in a way that changes gradually through time.
In our second example, the lags in N2O prediction were explained by the lags in soil moisture and soil mineral N prediction, with which they had a positive relationship.
After all micro elements reach a relaxed steady-state, measurements are obtained using a cumulative averaging technique to reduce noise.
Each micro element is divided into spatially-oriented bins in the y-direction in order to resolve the velocity and shear-stress profiles.
Velocity in each bin is measured using the Cumulative Averaging Method (CAM) [24], while the stress tensor field is measured using the Irving–Kirkwood relationship [25].
A least-squares polynomial fit to the data is performed, which helps reduce noise further.
The fit produces a continuous function that avoids stability issues arising from supplying highly fluctuating data to the macro solver.
A least-squares fit is applied to an Nth order polynomial for the velocity profile in the core region, and an Mth order polynomial for the velocity profile in the constrained region:(16)〈ui,core〉=∑k=1Nbk,iyi′(N−k),for 0⩽yi′⩽hcore, and(17)〈ui,cs〉=∑k=1Mck,iyi″(M−k),for 0⩽yi″⩽hcs, where bk,i and ck,i are the coefficients of the polynomials used in the core micro region and constrained region respectively.
An estimate of the new slip velocity uB for input to the macro solution (6) is taken directly from the compressed wall micro-element solution (16), at yi′=0.
In this paper a comparison between two popular feature extraction methods is presented.
Scale-invariant feature transform (or SIFT) is the first method.
The Speeded up robust features (or SURF) is presented as second.
These two methods are tested on set of depth maps.
Ten defined gestures of left hand are in these depth maps.
The Microsoft Kinect camera is used for capturing the images [1].
The Support vector machine (or SVM) is used as classification method.
The results are accuracy of SVM prediction on selected images.
The other methods for enhancement of photocatalytic activity are grafting co-catalysts.
There are two kinds of co-catalysts in terms of its function: one is for separation of electrons and the other is for separation of holes.
The former representative co-catalysts are Pt, Fe3+, and Cu2+ [9–12].
It was reported that Fe3+ and Cu2+ were grafted as amorphous oxide cluster [9,10], and reduced into Fe2+ and Cu+ by receiving one electron, respectively [11,12].
The reduced metal oxide cluster with reduced ions could return into the original state by giving more than one electron to molecular oxygen.
The latter ones are CoOx, CoPi (CoPOx), IrOx, and RuOx which are used for water oxidation, among which CoPi is reported to be the most effective co-catalyst for water oxidation [13].
However, there were few reports concerning co-grafting effects on photocatalytic activity especially in gaseous phase.
We expected that by co-grafting of both co-catalysts for separations of electrons and holes, photocatalytic activity in gaseous phase would be further enhanced.
Moreover, complex of BiVO4 with the other materials of p-type semiconductor is also effective for enhancing photocatalytic activity.
Although the basic mechanisms of the AD process are reasonably well understood, it has not proved simple to apply existing theories to the interpretation of experimental data.
What is needed is a combination of the AD theory and the electronic structure of realistic systems, including surface defects and adsorbed species.
Such electronic structure calculations are still complex and time-consuming.
In many cases, especially for insulating surfaces, attempts to model MIES spectra use simple or intuitive models.
In Refs.
[4,6,23] it is assumed that the main transition mechanism is Auger de-excitation, and the MIES spectra have been simulated by the surface density of states (DOS) projected on the surface oxygen ions of the uppermost surface layer using a Hartree–Fock method (the crystal code [24,25]) and a density functional theory (DFT) method (the cetep code [26]).
The effect of the overlap between the surface and He(1s) wavefunctions was taken into account only approximately by applying an additional z-dependent exponential factor to the surface DOS.
Other workers [5,6] estimated the AD transition probability using a DOS projected on to the projectile 1s atomic orbital.
However, they were not able to use state-of-the-art methods for the surface electronic structure.
Yet the success of the simplified treatments [4–6], especially for MIES features such as relative energies of the different peaks, suggests that real spectra are indeed related to the projection of the surface DOS on to the projectile orbital.
An OPE of VQCD(r) was developed in [3].
In this and the next paragraph, we review the content of that paper relevant to our analysis.
Within this framework, short-distance contributions are contained in the potentials, which are in fact the Wilson coefficients, while non-perturbative contributions are contained in the matrix elements that are organized in multipole expansion in r→ at r≪ΛQCD−1.
The following relation was derived: (16)VQCD(r)=VS(r)+δEUS(r),(17)δEUS=−ig2TFNC∫0∞dte−iΔV(r)t×〈r→⋅E→a(t)φadj(t,0)abr→⋅E→b(0)〉+O(r3).
VS(r) denotes the singlet potential.
δEUS(r) denotes the non-perturbative contribution to the QCD potential, which starts at O(ΛQCD3r2) in the multipole expansion.
ΔV(r)=VO(r)−VS(r) denotes the difference between the octet and singlet potentials; see [3] for details.
Intuitively VS(r) corresponds to VUV(r;μf) and δEUS(r) to VIR(r;μf).
We adopt dimensional regularization in our analysis; we also refer to hard cutoff schemes when discussing conceptual aspects.
When incompatible three component polymer chains are tethered at a junction point, the resultant star molecules of the ABC type are in a very frustrated field in bulk.
That is, their junction points cannot be aligned on two-dimensional planes but on one-dimensional lines, as schematically shown in Fig.
1.
Furthermore, when the chain length difference is not so large, the array of junction points tends to be straight and long one.
Consequently each domain with mesoscopic sizes becomes cylinders, and their cross sections could be conformed by polygons [28,29].
This is because three interfaces, A/B, B/C and C/A are likely to be flat since there exist no junction points at interfaces and therefore chain entropy contribution to the free energy of structure formation is considerably small comparing with regular block and graft copolymer systems.
As a matter of fact, Dotera predicted several tiling patterns by the diagonal bond method, a new Monte Carlo Simulation [30], while Gemma and Dotera pointed out that only three regular tilings, i.e., (6.6.6), (4.8.8) and (4.6.12) are permitted for three-branched molecules proposed as the “even polygon theorem” [31].
Shear horizontal (SH) ultrasound waves are guided waves (they have propagation properties affected by the geometry of the propagation medium), with symmetric and anti-symmetric modes; phase and group speeds are dependent on frequency, sample thickness, and the bulk shear wave speed [11,12].
The properties of the different modes can be very useful, such as in thickness measurement [13], but in this case they are a complication.
SH0 has a thickness independent speed, equal to the shear wave speed, and is non-dispersive (the phase and group speed are equal to the shear wave speed for all frequencies).
The oscillation direction of SH ultrasound is in the plane of the surface where the wave was generated, and perpendicular to the propagation direction, as shown in Fig.
1, with respect to a reference interface, which is typically a sample surface.
Under certain conditions, such as over short propagation distances, SH waves can be treated as bulk waves.
Fig.
11 shows the wear-mode map of RH ceramics, in which the early-stage friction coefficients and the surface roughness of the pure surface were chosen.
The value of the fracture toughness of RH ceramics was calculated based on the reference data in other literature [22].
The Sc of RH ceramics was smaller than Sc,critical under all tested conditions during the initial stage of friction.
Thus, the initial wear mode of RH ceramics was powder formation or plowing.
In addition, powder formation and plowing can be distinguished using a dimensionless parameter (Sc⁎) and a critical parameter (Sc,critical⁎).
(3)Sc⁎=HvRmaxKIc(4)Sc,critical⁎=51+10μwhere Hv is the Vickers hardness of RH ceramics [Pa].
The initial wear mode of RH ceramics was determined as powder formation under all tested conditions, as demonstrated in Fig.
12(a).
Furthermore, the wear-mode map at 2×104 cycles was constructed, as shown in Fig.
12(b).
In the map, all plots moved near the transition curve to plowing.
In particular, the plots for RH ceramics sliding against stainless steel or Al2O3 balls were nearer than SiC or Si3N4 balls.
Therefore, RH ceramics sliding against SiC and Si3N4 balls showed relatively higher wear than the other counterpart materials.
Nevertheless, these results from the wear-mode maps indicated that the wear mode of RH ceramics was powder formation accompanied with microcracks under all tested conditions in this study, resulting in low wear (<5×10−9mm2/N).
Indeed, the observation of the worn surfaces revealed that the catastrophic wear of RH ceramics accompanied by large brittle fracture was prevented overall, as shown in Fig.
13.
We order the discrete unknowns so that the vector of unknowns, xPS=[X,L], contains the nx unknown nodal coordinates, followed by the nb unknown discrete Lagrange multipliers.
The linear systems to be solved in the course of the Newton-based solution of Eq.
(10), subject to the displacement constraint (9), then have saddle-point structure,(15)where E is the tangent stiffness matrix of the unconstrained pseudo-solid problem, and the two off-diagonal blocks Cxl and Clx=CxlT arise through the imposition of the displacement constraint by the Lagrange multipliers.
We refer to [34] for the proof of the LBB stability of this discretisation; see also [35,36] for a discussion of the LBB stability of the Lagrange-multiplier-based imposition of Dirichlet boundary conditions in related problems.
We note that during the first step of the Newton iteration, E is symmetric positive definite since it represents the tangent stiffness matrix relative to the system’s equilibrium configuration.
Global optimisation algorithms are used in this study to solve the optimisation problem as they are known to be efficient in incorporating statistical information and dealing with complicated objective functions that have multiple local minima/maxima.
The genetic algorithm (GA) is such a global optimisation technique that mimics biological evolution processes and is used in this particular study.
The algorithm starts with a random selection of a population from the decision variable domain (X).
The genetic algorithm repeatedly modifies this population.
At each step, the algorithm selects a group of individual values from the population (parent) which are evolved through crossover or mutation to produce members of the next generation.
This process is repeated for several generations until an optimum solution is reached.
See [19] for a fuller description of the GA.
According to the ellipsometric spectra, optical constants and other physical parameters can be extracted by an appropriate fitting model.
In order to estimate the optical constants/dielectric functions of Ni-doped TiO2 films, a three-phase layered system (air/film/substrate) [15] was utilized to study the ellipsometric spectra.
TiO2 belongs to the wide band gap semiconductors.
Considering the contribution of the M0 type critical point with the lowest three dimensions, its dielectric function can be calculated by Adachi's model [15,22,23]: ε(Ε)=ε∞+{A0[2−(1+χ0)1/2−(1−χ0)1/2]}/(EOBG2/3χ02).
In the model, E is the incident photon energy, ε∞ is the high-frequency dielectric constant, χ0=(E+iΓ), EOBG is the optical gap energy, and A0 and Γ are the strength and broadening parameters of the EOBG transition, respectively.
As an example, the experimental SE of the film TN1 at an incident angle 70° by dot scatter is shown in Fig.
4.
The Fabry–Pérot interference oscillations due to multiple reflections within the film have been found in the photon energy from 1.5eV to 3.5eV (354nm–826nm), which indicates that the films are transparent in this region.
Note that a good agreement of the experimental and calculated spectra is attained in the whole measured photon energy range.
The fitting thickness for film TN2 is 159nm, which is very near to the value obtained by SEM (see Fig.
1(b)).
There are a number of avenues to explore for future work, in particular the use of other time–frequency analysis methods.
The STFT spectrogram was utilised here, as it is the simplest to implement.
Whilst all of the echoes could be clearly resolved in both time and frequency, the spectrogram suffers from a fixed resolution, i.e.
an increase of time resolution necessarily leads to a decrease in frequency resolution.
Other methods of time–frequency analysis, such as discrete wavelet analysis, benefit from advantage of multi-resolution analysis, which offers improved temporal resolution of the high frequency components, and frequency resolution of the low frequency components [25,18,19].
Also, whilst the current work has utilised SH waves that are generated by EMATs, the physics that describes the pulsed array system is universal to other types of waves.
Future work will include demonstrating this phenomenon with a number of other systems, for example using longitudinal ultrasonic waves or electromagnetic waves.
Xylanases have potential applications in various fields.
Some of the important applications are as fallows.
Xylanases are used as bleaching agent in the pulp and paper industry.
Mostly they are used to hydrolyzed the xylan component from wood which facilitate in removal of lignin (Viikari, Kantelinen, Buchert, & Puls, 1994).
It also helps in brightening of the pulp to avoid the chlorine free bleaching operations (Paice, Jurasek, Ho, Bourbonnais, & Archibald, 1989).
In bakeries the xylanase act on the gluten fraction of the dough and help in the even redistribution of the water content of the bread (Wong & Saddler, 1992).
Xylanases also have potential application in animal feed industry.
They are used for the hydrolysis of non-starchy polysaccharides such as arabinoxylan in monogastric diets (Walsh, Power, & Headon, 1993).
Xylanases also play a key role in the maceration of vegetable matter (Beck & Scoot, 1974), protoplastation of plant cells, clarification of juices and wine (Biely, 1985) liquefaction of coffee mucilage for making liquid coffee, recovery of oil from subterranian mines, extraction of flavors and pigments, plant oils and starch (McCleary, 1986) and to improve the efficiency of agricultural silage production (Wong & Saddler, 1992).
A sentence alignment model based on combined clues and Kernel Extensional Matrix Matching (KEMM) method is proposed.
In this model, a similarity matrix for sentence aligning is formed by the similarities of bilingual sentences calculated by the combined clues, such as lexicon, morphology, length and special symbols, etc.
; then this similarity matrix is used to construct a select matrix for sentence aligning; finally, obtains the sentence alignments by KEMM.
Experimental results illustrated that our model outperforms over the Gale's system on handling any types of sentence alignments, with 30% total sentence alignment error rate decreasing.
A hydroxyl-functionalized poly(butylene succinate) based polyester was prepared by conventional polycondensation of benzyl-protected dimethyl malonate and 1,4-butanediol (Scheme 2(a)) [24a].
Yao et al.
reported on the direct polycondensation of l-lactic acid and citric acid with the formation of poly[(l-lactic acid)-co-(citric acid)], obtaining a polyester oligomer with both pendant carboxylic and hydroxyl groups [24b].
This PLCA oligomer was reacted with dihydroxylated PLLA as a macromonomer, yielding a PLCA–PLLA multiblock copolymer as shown in Scheme 2(b).
While lipases have been investigated for the ring-opening polymerization (ROP) of cyclic ester monomers [25,26], they have also been used for the preparation of polyesters by polycondensation reactions.
The advantage of this technique is that these enzyme-catalyzed reactions proceed without protection of the pendant functional groups.
In this field, hydroxyl-bearing polyesters have been synthesized by the copolymerization of divinyl adipate with various triols (e.g.
glycerol, 1,2,4-butanetriol) as represented in Scheme 2(c) [27] and by copolymerizations of 1,8-octanediol with adipic acid and several alditols [28].
Very recently, several α-hydroxy acids derived from amino acids were homo- and copolymerized with lactic acid by polycondensation in bulk without protected monomers (Scheme 2(d)) [29].
Biodegradable polyesters with various pendant groups were obtained, although the molecular weights remained low (1000–3000gmol−1).
The purported advantages of EMR implementation in urban slums are widely promoted.
Increasingly capable health information systems could facilitate communication, help coordinate care, and improve the continuity of care in disadvantaged communities like Kibera.
However, available systems may not have the ability to simplify care or improve efficiency where funding and human resources are scarce, infrastructure is unreliable and health data demands are opportunistic, not strategic.
This study described perceptions of local EMR stakeholders in two urban slum clinics.
They shared many observations that may be important for other EMR initiatives to heed, and worried most about the sustainability of EMR initiatives in like communities.
The future for EMR use in urban slums is promising.
Innovative new technologies, such as mobile applications and point-of-care laboratory tests, could extend the reach of EMRs where infrastructure is wanting.
New cloud-based EMR ecosystems, where data is collected and stored centrally could leverage cell phone networks to promote more health information sharing, coordination of care and ultimately better outcomes for vulnerable populations.Summary pointsWhat was already known on the topic?•Rapid urbanization is associated with growth in the number and size of urban slums and an associated rise in the burden of disease, further worsening an already fragmented and inefficient health care system.
Volume EM can be performed using transmission or scanning electron microscopes.
Each approach has its own strengths and weaknesses, and the choice is dependant on the required lateral (x, y) and axial (z) resolution, and the size of the structure of interest.
Historically, transmission electron microscopy (TEM) was the tool of choice for ultrastructural examination of biomedical specimens at sub-nanometer resolution.
However, for many cell biology studies structural resolution is actually limited by the deposition of heavy metals onto membranes during sample preparation.
In addition, voxel dimensions may only need to be half that of the smallest expected feature of interest (Briggman and Bock, 2012).
Advances in scanning electron microscopy (SEM) technology are now driving a paradigm shift in electron imaging.
SEMs with field emission electron sources and high efficiency electron detectors can achieve lateral resolutions in the order of 3nm, allowing visualisation of structures such as synaptic vesicles and membranes (De Winter et al., 2009; Knott et al., 2008; Vihinen et al., 2013; Villinger et al., 2012), though resolving individual leaflets of membrane bilayers remains a challenge (Vihinen et al., 2013).
The use of low beam energies also limits the interaction volume, enhancing axial resolution (Hennig and Denk, 2007).
In this review, volume imaging in both transmission and scanning EMs will be explored, moving from traditional manual techniques, through to the latest systems where aspects of both sample preparation and imaging have been automated.
Three-dimensional digital subtraction angiographic (3D-DSA) images from diagnostic cerebral angiography were obtained at least one day prior to embolization in all patients.
The raw data of 3D-DSA in a DICOM file were used for creating a 3D model of the target vessel segment.
These data were converted to standard triangulation language (STL) surface data as an aggregation of fine triangular meshes using 3D visualization and measurement software (Amira version X, FEI, Burlington, MA, USA).
An unstructured computational volumetric mesh was constructed from the triangulated surface.
Smoothing and remeshing followed as next steps.
The STL file was then transferred to a 3D printer (OBJET30 Pro; Stratasys Ltd., Eden Prairie, MN, USA).
The resolution of the build layer was 0.028mm, and the 3D printed vessel model was produced using acrylic resin (Vero).
Following immersion in water for a few hours, the surface of the 3D printed model was smoothed by manually removing spicule.
When we formulate the downscaling problem as a multi-objective optimization problem, we face, however, the following problems.
Minimizing the sum of different objectives is problematic, since they may have different units and ranges.
Even with an appropriate scaling procedure there is a risk of treating the objectives unequally or getting trapped in a local minimum.
Firstly, we can never know, what is the minimum value of each objective that can be achieved by the regression.
Thus, designing an appropriate scaling procedure is difficult and one would need to decide on the relative importance of the different objectives in advance.
Secondly, adding multiple, conflicting objectives very likely results in a fitness function with multiple local minima, which makes optimization more difficult.
To avoid these problems, we have implemented fitness calculation according to the Strength Pareto Evolutionary Algorithm (SPEA) by Zitzler and Thiele (1999), instead of using a single (weighted) fitness or cost function.
Approaches for multi-objective optimization like SPEA are widely used in evolutionary computation.
In SPEA the fitness calculation during the fitting procedure is based on an intercomparison of the different models.
Further, a finite set of so called Pareto optimal models (downscaling rules) is returned.
One way to enforce this ratio is to use a probabilistic, ‘roulette wheel’ style lane selection policy.
VISSIM, along with most simulation toolkits, offers methods to specify probabilistic routing whereby a defined percentage of vehicles are sent down unique routes.
This is a piecewise technique that can be reapplied at various locations around a simulation.
While these methods are attractive from a calibration perspective as exact representations of existing statistics can be ensured, the process is an unrealistic one as it assumes that drivers make probabilistic decisions at precise locations.
So in this case when a vehicle arrives at a point prior to the weighbridges it is allocated one of the lanes based on the respective probabilities.
It turns out that this method leads to significant variations in trip times depending on the initial random number seed, this can be seen in a graphic of the key areas of the simulation for the 2 different runs (Fig.
7).
One of the benefits of graphical microsimulation is that the 2D and 3D simulations help the researcher to visualise a new scheme and its potential benefits but also to highlight unrealistic behaviour.
Fig.
7 shows the congestion at the decision point for 2 different runs.
Using probabilistic routing to enforce correct routing percentages is a clear case of overcalibration affecting simulation brittleness.
The Discrete Element Method applied to spheres is well established as a reasonably realistic tool, in a wide range of engineering disciplines, for modelling packing and flow of granular materials; Asmar et al.
[8] describes the fundamentals of this method as applied by code developed in-house at Nottingham; since these are widely documented the details are not reproduced here, simply a summary.
It applies an explicit time stepping approach to numerically integrate the translational and rotational motion of each particle from the resulting forces and moments acting on them at each timestep.
The inter-particle and particle wall contacts are modelled using the linear spring–dashpot–slider analogy.
Contact forces are modelled in the normal and tangential directions with respect to the line connecting the particles centres.
Particle elastic stiffness is set so sphere “overlap” is not significant and moderate contact damping is applied.
Particle cohesion can also be modelled but is assumed to be negligible in the current study.
The translational and rotational motion of each particle is modelled using a half step leap-frog Verlet numerical integration scheme to update particle positions and velocities.
Near-neighbour lists are used to increase the computational efficiency of determining particle contacts and a zoning method is used each time the list is composed; that is the system is divided into cubic regions, each particle centre is within one zone, and potential contacting particles are within the same or next-door neighbour zones.
Full details are given in Asmar et al.
[8].
Superconductivity in actinides was first observed in thorium metal in 1929 [7], then in elemental uranium in 1942 [8], and in uranium compounds in 1958 [9].
A new class of uranium superconductors emerged in the 1980's with the discovery of uranium heavy fermion superconductors [10].
Further surprises came at the beginning of the century with the discovery of ferromagnetic superconductors in uranium systems [11] and the first observation of superconductivity in plutonium [12] and neptunium [13] compounds.
The actinides (or actinoids) are located at the end of the periodic table (N=89 (Ac) or 90 (Th) to 103 (Lr)).
Transuranium elements (or transuranics) are the chemical elements with atomic number (Z) greater than 92 (uranium) and due to their short half-life on a geological timescale, they are essentially synthetic elements.
Above Z=103 (Lr), one talks about transactinides (or superactinides) elements.
These latter elements have extremely short half-lives and no macroscopic quantity is available for the study of condensed-matter properties.
In this section, we use the terrain data processing as an example to describe the geodetic data transformation method.
Since Google Maps/Earth server only gives the terrain data in graphical display, we have to get terrain digital data from other sources.
The fine-resolution (3″ or finer) terrain data bases such as SRTM (Shuttle Radar Topographical Mission) or USGS's DEM (Digital Elevation Model) data are necessary.
Moreover, since 3DWF is used to model the fine-scale (meters up to 100m) atmospheric flow, it needs fine resolution terrain data.
In this project, we use the terrain elevation data set from SRTM (Farr et al.
2007) with 3-arcsecond (~90m resolution at the equator) resolution.
The data covers the land area, nearly global from 56S to 60N latitudes.
We use the processed version 4 SRTM data set as described in Gamache (2005) in which some of the missing data holes were filled.
The original data is organized in WGS84 (World Geodetic System 84) geodetic coordinate system.
When the data are applied to the 3DWF model, they are transformed to the local East, North and Up (ENU) coordinate (see Fig.
3).
Since the 3DWF is a fine scale wind model and its entire model domain is not intended to be larger than 20×20km, this Cartesian coordinate system is a good choice with very little distortion due to the curvature of the Earth's surface.
The transformation from the WGS84 data to the ENU coordinate is performed as follows (Fukushima, 2006; Featherstone and Claessens, 2008).
Fig.
7 shows the relationship between the testing time and friction coefficients of various samples under dry conditions.
There exist running in and steady wear period in the wear process of uncoated AZ31 and anodizing coating without Al2O3 nanoparticles while there has a steady wear period only in the wear process of composite anodizing coating with Al2O3 nanoparticles.
At the same time, the addition of nano-particles to electrolyte led to reduction of friction coefficient.
The friction coefficient of composite coating is relatively lower and more stable than what has been reported in literature [24,25] for anodizing coatings.
This may be caused by “rolling effect” made by Al2O3 nanoparticles on the surface of oxide coating.
Spherical nanoparticles change sliding into rolling, which reduce friction, making the friction coefficient becomes more stable.
The friction coefficient of anodizing coating without Al2O3 nanoparticles has large fluctuation maybe for the damage of coating.
In contrast to the uncoated AZ31 magnesium alloy, the anodizing coatings show slightly lower friction coefficient.
This can be attributed to their higher load-bearing capacity for high hardness.
Despite the loss of directed, self-complementary hydrogen bonding through alkylation of the imidazole ring, electrostatic aggregation of imidazolium salts is a tunable, self-assembly process, which is instrumental to several applications.
Imidazolium salts are used to extract metal ions from aqueous solutions and coat metal nanoparticles [15], dissolve carbohydrates [16], and create polyelectrolyte brushes on surfaces [17].
For example, atom transfer radical polymerization (ATRP) was used to graft poly(1-ethyl 3-(2-methacryloyloxy ethyl) imidazolium chloride) brushes onto gold surfaces [17].
One of the imidazolium salt’s most promising attributes is its antimicrobial action [12,18] and molecular self-assembly into liquid crystals [19,20].
1-Alkyl-3-methylimidazolium chlorides and bromides, 1-alkyl-2-methyl-3-hydroxyethylimidazolium chlorides, and N-alkyl-N-hydroxyethylpyrrolidinonium, for example, all exhibit strong biocidal activity [18].
Hydrogels form from polymerized methylimidazolium-based ionic liquids with acryloyl groups; the polymer self-assembles into organized lamellae with unique swelling properties, leading to bioactive applications [19].
Other bioactive applications for imidazolium salts include antiarrhythmics [21], anti-metastic agents [22,23], and imidazolium-based steroids [24].
Separation applications include efficient absorption of CO2 [25].
Imidazolium salts enhance vesicle formation as imidazolium surfactants [26], and they also find application in polymeric actuators [27].
The test cases confirm that the high-order discretisation retains exponential convergence properties with increasing geometric and expansion polynomial order if both the solution and true surface are smooth.
Errors are found to saturate when the geometric errors, due to the parametrisation of the surface elements, begin to dominate the temporal and spatial discretisation errors.
For the smooth solutions considered as test cases, the results show that this dominance of geometric errors quickly limits the effectiveness of further increases in the number of degrees of freedom, either through mesh refinement or higher solution polynomial orders.
Increasing the order of the geometry parametrisation reduces the geometric error.
The analytic test cases presented here use a coarse curvilinear mesh; for applications, meshes are typically more refined in order to capture features in the solution and so will better capture the geometry and consequently reduce this lower bound on the solution error.
If the solution is not smooth, we do not expect to see rapid convergence.
In the case that the solution is smooth, but the true surface is not, then exponential convergence with P and Pg can only be achieved if, and only if, the discontinuities are aligned with element boundaries.
However, if discontinuities lie within an element, convergence will be limited by the geometric approximation, since the true surface cannot be captured.
In the cardiac problem, we consider both the true surface and solution to be smooth.
We addressed the question whether carbohydrate coupling increased antigen uptake by DCs via C-type lectin receptor targeting.
Therefore, the antigens were labeled with pHrodo Red dye (Invitrogen), a dye that specifically fluoresces as pH decreases from neutral to acidic, as provided in endosomes/lysosomes of cells.
In vitro characterization of the cellular uptake of neoglycocomplexes using bone marrow derived dendritic cells (BMDCs) demonstrated superior ingestion of mannan-conjugates MN–Ova and MN–Pap (Supplementary Fig.
S4A-D,F).
This was confirmed in vivo by intradermal needle-injection of labeled antigen into the ear pinnae of mice.
Antigen uptake and transport to the ear dLNs were measured after 24h by FACS analysis.
DCs in cervical LNs were identified according to their high expression of MHC class II (Fig.
3A) and additionally characterized by CD8α, CD11b, and CD11c expression and uptake of pHrodo-labeled antigen (Fig.
3B, D–F).
The results showed significantly elevated numbers of pHrodo+MHCIIhigh DCs for mannan conjugates MN–Ova and MN–Pap (and to a lesser degree for MD–Pap) in comparison to the unmodified antigens (Fig.
3C).
Both carbohydrates targeted antigen preferentially to CD8α− DCs, as indicated by an increase in CD8α−/pHrodo+ DCs compared to unmodified antigens (Fig.
3E and F).
Nevertheless, whether the antigens were taken up in situ by dermal DCs or by LN resident APCs via the afferent lymphatics could not be fully elucidated.
Histology revealed that antigen-loaded cells in the dLNs were already present 30min after intradermal injection (Supplementary Fig.
S4G), suggesting both mechanisms.
Let us now consider the case of a beta-beam source.
Similarly to the case of a static tritium source, an advantage of the beta-beams is that the neutrino fluxes can be very accurately calculated.
Fig.
3 shows the electron–neutrino scattering events in the range of 0.1 MeV to 1 MeV and 1 keV to 10 keV, respectively.
(In Fig.
3(b) we have rounded to the nearest integer number of counts.)
The shape of the flux-averaged cross sections is very similar to the reactor case as reflected in the event rates shown in the figures.
As can be seen, by measuring electron recoils in the keV range with a beta-beam source one could, with a sufficiently strong source, have a very clear signature for a neutrino magnetic moment of 5×10−11μB.
These figures are for Helium-6 ions, however, similar results can be obtained using neutrinos from 18Ne.
The results shown are obtained for an intensity of 1015 ν/s (i.e., 1015  ions/s).
If there is no magnetic moment, this intensity will produce about 170 events in the 0.1 MeV to 1 MeV range per year and 3 events in the 1 keV to 10 keV range per year.
These numbers increase to 210 and 55, respectively, in the case of a magnetic moment of 5×10−11μB.
Plastically deformed MGs develop inhomogeneity and show harder and softer regions [16].
While this could in principle be associated with a BE according to the composite model, a MG provides no basis for a dislocation-based theory.
The search for a BE in plastic flow is hindered by the softening of MGs associated with shear-banding (in contrast to the work-hardening familiar in conventional alloys).
Anelastic deformation is, however, of interest as its time-dependence must relate to relaxation processes in the MG structure that in turn should be connected to the onset of plasticity.
In particular, anelasticity may offer a way to study the operation of the shear transformation zones (STZs [17]) often used to interpret the deformation of MGs.
Fujita et al.
have used torsion tests to observe anelasticity in MGs loaded (at maximum, on the cylindrical sample surface) to 30%, 16% and just 4% of the shear yield stress τy [18].
In the present work we apply torsion to MG samples to reach stresses up to 24% of τy, and for the first time in the elastic regime investigate the effects of torque reversal.
Myocardial electrical propagation can be simulated using the monodomain or bidomain PDEs [5,6].
Due to its capacity to represent complex geometries with ease, approximations are often obtained using the finite element method (FEM) to discretise the PDEs in space on realistic cardiac geometry meshes; this results in very large (up to forty-million degrees of freedom (DOF) for human heart geometries) systems of linear equations which must be solved many thousands of times over the course of even a short simulation.
Thus, they are extremely computationally demanding, presenting taxing problems even to high-end supercomputing resources.
This computational demand means that effort has been invested in developing efficient solution techniques, including work on preconditioning, parallelisation and adaptivity in space and time [7–12].
In this study, we investigate the potential of reducing the number of DOF by using a high-order polynomial FEM [13–15] to approximate the monodomain PDE in space, with the goal of significantly improving simulation efficiency over the piecewise-linear FEM approach commonly used in the field [16–19].
For schemes where the polynomial degree p of the elements is adjusted according to the error in the approximation, this is known as the finite element p-version.
In the work presented here, we work with schemes which keep p fixed.
The proposed multihop routing protocol, PHASeR, applies the technique of blind forwarding in a MWSN, which increases the reliability of data delivery through its inherent use of multiple routes.
This approach requires a gradient metric to be continuously maintained, which is problematic in a dynamic topology.
The literature commonly uses either flooding or location awareness, however flooding creates large amounts of overhead and location determination schemes can often be inaccurate, power hungry and create the issue of the dead end problem.
PHASeR uses a novel method of gradient maintenance in a mobile network, which requires the proactive sharing of only local topology information.
This is facilitated by a global TDMA (time division multiple access) MAC (medium access control) layer and further reduces the amount of overhead, which in turn will decrease packet latency.
PHASeR is also set apart by its use of encapsulation, which allows data from multiple nodes to be transmitted in the same packet in order to handle high volumes of traffic.
It utilises node cooperation to create a robust multipath routing solution.
As such, the contribution of this paper is a cross-layer routing protocol for MWSNs that can handle the constant flow of data from sensors in highly mobile situations.
A principle of high-throughput materials science is that one does not know a priori where the value of the data lies for any specific application.
Trends and insights are deduced a posteriori.
This requires efficient interfaces to interrogate available data on various levels.
We have developed a simple WEB-based API to greatly improve the accessibility and utility of the AFLOWLIB database [14] to the scientific community.
Through it, the client can access calculated physical properties (thermodynamic, crystallographic, or mechanical properties), as well as simulation provenance and runtime properties of the included systems.
The data may be used directly (e.g., to browse a class of materials with a desired property) or integrated into higher level work-flows.
The interface also allows for the sharing of updates of data used in previous published works, e.g., previously calculated alloy phase diagrams [19–31], thus the database can be expanded systematically.
In the Total Focusing Method (TFM) the beam is synthetically focused at every point in the target region [7] as follows.
After obtaining the FMC data, the target region, which is in the x–z plane in 2D (Fig.
1), is discretized into a grid.
The signals from all elements in the array are then summed to synthesize a focus at every point in this grid.
Linear interpolation of the time domain signals is necessary since they are discretely sampled.
The intensity of the TFM image ITFM at any point (x,z) is given by:(10)ITFM(x,z)=|∑HTR(1c((xT−x)2+z2+(xR−x)2+z2))|forallT,Rwhere HTR(t) is the Hilbert transform of a signal uTR(t) in the FMC data, xT is the x-position of the transmitting element (T) and xR is the x-position of the receiving element (R).
Note that the z-position of all elements is zero (Fig.
3a).
The summation is carried out for all possible transmitter–receiver pairs and therefore uses all the information captured with FMC.
This algorithm is referred to as ‘conventional TFM’ in this paper.
In this paper, we present our experimental observations on how solvents can vary the TPA and TPF properties of fluorescent rhodamine (Rh) dyes Rh6G, RhB and Rh101.
Rhodamines are well-known xanthenes dyes, which have been extensively used for many widespread applications in single-molecule detection [24], DNA-sequence determination [25], fluorescence labelling [26], etc.
due to their strong fluorescence over the visible spectral region.
Molecular geometries of rhodamine dyes are well-known [27,28] and indicate that all the structures are non-centrosymmetric.
In general, for centrosymmetric molecules, TPA is forbidden when tuned to the transitions at one-half of the excitation frequencies.
However, for non-centrosymmetric molecules due to symmetry relaxations, the single-photon absorption (SPA) peaks and TPA peaks may coincide.
So we set our primary aim to find the effect of solvent polarity on the correlation of SPA and TPA peaks for all the dyes.
It has recently been demonstrated [15] (see also [13] and references therein) that for a self-dual background the two-loop QED effective action takes a remarkably simple form that is very similar to the one-loop action in the same background.
There are expectations that this similarity persists at higher loops, and therefore there should be some remarkable structure encoded in the all-loop effective action for gauge theories.
In the supersymmetric case, one has to replace the requirement of self-duality by that of relaxed super self-duality [16] in order to arrive at conclusions similar to those given in [15].
Further progress in this direction may be achieved through the analysis of N=2 covariant supergraphs.
Finally, we believe that the results of this Letter may be helpful in the context of the conjectured correspondence [17–19] between the D3-brane action in AdS5×S5 and the low-energy action for N=4 SU(N) SYM on its Coulomb branch, with the gauge group SU(N) spontaneously broken to SU(N−1)×U(1).
There have appeared two independent F6 tests of this conjecture [19,20], with conflicting conclusions.
The approach advocated here provides the opportunity for a further test.
Contact methods have been developed and used in Lagrangian staggered-grid hydrodynamic (SGH) calculations for many years.
Early examples of contact methods are discussed in Wilkins [37] and Cherry et al.
[7].
Hallquist et al.
[17] provides an overview of multiple contact algorithms used in various Lagrangian SGH codes dating back to HEMP [37].
Of particular interest, Hallquist et al.
[17] describes the contact surface scheme used in TOODY [31] and later implemented in DYNA2D [36].
The contact method of TOODY uses a master–slave approach.
The goal of this approach is to treat the nodes on the contact surface in a manner similar to an internal node.
The physical properties of the slave surface are interpolated to a ghost mesh (termed phony elements in [17]) that overlays the slave zones.
The physical properties are interpolated from the slave surface to the ghost zones using surface area weights.
The surface area weights are equal to the ratio of the ghost zone surface area to the surface area of the master surface.
The contact surface method for nodal-based Lagrangian cell-centered hydrodynamics (CCH) presented in this paper will use surface area weights similar in concept to those in TOODY.
Following the area fraction approach of TOODY may seem retrospective; however, using surface area weights naturally extends to the new CCH methods that solve a Riemann-like problem at the node of a zone [10,24,25,3].
The expression for Pc is also easily found in the same basis, where it becomes apparent that the dynamics of conversion in matter depends only on the relative orientation of the eigenstates of the vacuum and matter Hamiltonians.
This allows to directly apply the known analytical solutions for Pc, and, upon rotating back, obtain a generalization of these results to the NSI case.
For example, the answer for the infinite exponential profile [18,19] A∝exp(−r/r0) becomes Pc=exp[γ(1−cos2θrel)/2]−1exp(γ)−1, where γ≡4πr0Δ=πr0Δm2/Eν.
We further observe that since γ⪢1 the adiabaticity violation occurs only when |θ−α|⪡1 and φ≃π/2, which is the analogue of the small-angle MSW [10,20] effect in the rotated basis.
The “resonant” region in the Sun where level jumping can take place is narrow, defined by A≃Δ [21].
A neutrino produced at a lower density evolves adiabatically, while a neutrino produced at a higher density may undergo level crossing.
The probability Pc in the latter case is given to a very good accuracy by the formula for the linear profile, with an appropriate gradient taken along the neutrino trajectory, (12)Pc≃Θ(A−Δ)e−γ(cos2θrel+1)/2, where Θ(x) is the step function, Θ(x)=1 for x>0 and Θ(x)=0 otherwise.
We emphasize that our results differ from the similar ones given in [5,22] in three important respects: (i) they are valid for all, not just small values of α (which is essential for our application), (ii) they include the angle φ, and (iii) the argument of the Θ function does not contain cos2θ, as follows from [21].
We stress that for large values of α and φ≃π/2 adiabaticity is violated for large values of θ.
Recent astronomical observations of high redshift type Ia supernovae performed by two groups [1–3] as well as the power spectrum of the cosmic microwave background radiation obtained by the BOOMERANG [4] and MAXIMA-1 [5] experiments seem to indicate that at present the Universe is in a state of accelerated expansion.
If one analyzes these data within the Friedmann–Robertson–Walker (FRW) standard model of cosmology their most natural interpretation is that the Universe is spatially flat and that the (baryonic plus dark) matter density ρ is about one third of the critical density ρcrit.
Most interestingly, the dominant contribution to the energy density is provided by the cosmological constant Λ.
The vacuum energy density (1.1)ρΛ≡Λ/(8πG) is about twice as large as ρ, i.e., about two thirds of the critical density.
With ΩM≡ρ/ρcrit, ΩΛ≡ρΛ/ρcrit and Ωtot≡ΩM+ΩΛ: (1.2)ΩM≈1/3,ΩΛ≈2/3,Ωtot≈1.
This implies that the deceleration parameter q is approximately −1/2.
While originally the cosmological constant problem [6] was related to the question why Λ is so unnaturally small, the discovery of the important role played by ρΛ has shifted the emphasis toward the “coincidence problem”, the question why ρ and ρΛ happen to be of the same order of magnitude precisely at this very moment [7].
Nanoparticle Tracking Analysis (NTA) has been applied to characterising soot agglomerates of particles and compared with Transmission Electron Microscoscopy (TEM).
Soot nanoparticles were extracted from used oil drawn from the sump of a light duty automotive diesel engine.
The samples were prepared for analysis by diluting with heptane.
Individual tracking of soot agglomerates allows for size distribution analysis.
The size of soot was compared with length measurements of projected two-dimensional TEM images of agglomerates.
Both the techniques show that soot-in-oil exists as agglomerates with average size of 120nm.
NTA is able to measure particles in polydisperse solutions and reports the size and volume distribution of soot-in-oil aggregates; it has the advantages of being fast and relatively low cost if compared with TEM.Nanoparticle Tracking Analysis (NTA) has been applied to characterising soot agglomerates of particles and compared with Transmission Electron Microscoscopy (TEM).
Soot nanoparticles were extracted from used oil drawn from the sump of a light duty automotive diesel engine.
The samples were prepared for analysis by diluting with heptane.
Individual tracking of soot agglomerates allows for size distribution analysis.
The size of soot was compared with length measurements of projected two-dimensional TEM images of agglomerates.
Both the techniques show that soot-in-oil exists as agglomerates with average size of 120nm.
NTA is able to measure particles in polydisperse solutions and reports the size and volume distribution of soot-in-oil aggregates; it has the advantages of being fast and relatively low cost if compared with TEM.
Numerical simulation of the gas flow through such non-trivial internal geometries is, however, extremely challenging.
This is because conventional continuum fluid dynamics, which assumes that locally a gas is close to a state of thermodynamic equilibrium, becomes invalid or inaccurate as the smallest characteristic scale of the geometry (e.g.
the channel height) approaches the mean distance between molecular collisions, λ [1].
An accurate and flexible modelling alternative for these cases is the direct simulation Monte Carlo method (DSMC) [2].
However, DSMC can be prohibitively expensive for internal-flow applications, which typically have a geometry of high-aspect ratio (i.e.
are extremely long, relative to their cross-section).
The high-aspect ratio creates a formidable multiscale problem: processes need to be resolved occurring over the smallest characteristic scale of the geometry (e.g.
a channelʼs height), as well as over the largest characteristic scale of the geometry (e.g.
the length of a long channel network), simultaneously.
In this paper, an implementation of a LBP (local binary pattern) based fast face recognition system on symbian platform is presented.
First, face in picture taken from camera is detected using AdaBoost algorithm.
Second, the pre-processing of the face is done, including eye location, geometric normalization, illumination normalization.
During the face preprocessing, a rapid eye location method named ER (Eyeball Search) is proposed and implemented.
Last, the improved LBP is adopted for recognition.
Although the computational capability of the symbian platform is limited, the experimental results show good performance for recognition rate and time.
in pressIn this paper, an implementation of a LBP (local binary pattern) based fast face recognition system on symbian platform is presented.
First, face in picture taken from camera is detected using AdaBoost algorithm.
Second, the pre-processing of the face is done, including eye location, geometric normalization, illumination normalization.
During the face preprocessing, a rapid eye location method named ER (Eyeball Search) is proposed and implemented.
Last, the improved LBP is adopted for recognition.
Although the computational capability of the symbian platform is limited, the experimental results show good performance for recognition rate and time.
in press
We define a new multispecies model of Calogero type in D dimensions with harmonic, two-body and three-body interactions.
Using the underlying conformal SU(1,1) algebra, we indicate how to find the complete set of the states in Bargmann–Fock space.
There are towers of states, with equidistant energy spectra in each tower.
We explicitely construct all polynomial eigenstates, namely the center-of-mass states and global dilatation modes, and find their corresponding eigenenergies.
We also construct ladder operators for these global collective states.
Analysing corresponding Fock space, we detect the universal critical point at which the model exhibits singular behavior.
The above results are universal for all systems with underlying conformal SU(1,1) symmetry.
Measuring and analysing the hold time of the CPA pill allows the thermal boundary resistance within the pill to be assessed; the thermal boundary dictates the actual temperature of the CPA crystals in comparison to the temperature of the cold finger, which is maintained at a constant temperature by a servo control program.
Fig.
17 shows the temperature profile during the recycling of the CPA pill and subsequent operation at 200mK.
During the hold time, the servo control program maintained the CPA pill temperature to within a millikelvin.
It is expected that microkelvin stability can be achieved with fast read-out thermometry (which was not available at the time of testing but which will be used for the mKCC), as this would allow for temperature control on much faster (millisecond) timescales than the current (approximately 1s) thermometry readout used.
The threshold values for removing large caters were determined by examining the craters within the study area, referencing previous studies (Molloy and Stepinski, 2007), and some trial and error.
After the parameter values are determined, the rest of the process is automated.
However, we do anticipate some minimum manual editing may be needed in some complicated terrains when apply it to all of Mars.
To minimize the distortion resulted from map projection on global datasets, we will choose an equal area projection by evaluating the options suggested in Steinwand et al.
(1995) or conduct geodesic area calculation using software such as “Tools for Graphics and Shapes” (http://www.jennessent.com/arcgis/shapes_graphics.htm)Although post-formational modification to the valleys may be minimum (Williams and Phillips, 2001), there may nonetheless be modifications such as eolian fill and mass wasting (e.g., Grant et al., 2008).
Thus the volume estimates derived with PBTH method represents a lower bound.
Comparing the estimates from MOLA and HRSC data reveals that MOLA estimate is about 91% of HRSC value.
However, MOLA has global coverage whereas HRSC does not.
Therefore, for areas where there is only MOLA coverage, the estimate may be scaled upward by 1.1 times.
The algorithm has been tested on DEMs with various resolutions (2 m for simulated DEM, 75m for HRSC, and 463m for MOLA).
It can certainly be applied to higher resolution DEMs for Mars when they become available, but the threshold values will need to be adjusted.
Three Runge–Kutta IMEX schemes were tested by Ullrich and Jablonowski [23] for the HEVI solution of the equations governing atmospheric motion.
They tested the ARS(2,3,2) scheme of Ascher et al.
[1] and also suggested the less computationally expensive but nearly as accurate Strang carryover scheme.
This involves Strang splitting but the first implicit stage is cleverly re-used from the final implicit stage of the previous time-step and so there is only one implicit solution per time-step.
Another novel approach taken by Ullrich and Jablonowski [23] is to use a Rosenbrock solution in order to treat all of the vertical terms implicitly rather than just the terms involved in wave propagation.
A Rosenbrock solution is one iteration of a Newton solver.
This circumvents the time-step restriction associated with vertical advection at the cost of slowing the vertical advection.
Since perturbative expansion is used, it is impossible to find the exact bounds; instead, one can derive tree-level unitarity bounds or loop-improved unitarity bounds.
In this study, we will use unitarity bounds coming from a tree-level analysis [20].
This tree level analysis is derived with the help of the equivalence theorem [21], which itself is a high-energy approximation where it is assumed that the energy scale is much larger than the Z0 and W± gauge-boson masses.
We will consider here this “high-energy” hypothesis that both the equivalence theorem and the decoupling regime are well settled, but in such a way that the unitarity constraint is also fulfilled.
Our purpose is to investigate the quantum effects in the decays of the light CP-even Higgs boson h0, especially looking for sizeable differences with respect to the SM in the decoupling regime.
Recently together with structural efficiency, passenger safety is also an important issue in application of material to transportation industries.
Hence, the crashworthiness parameters are introducing to predict the capability of structure to prevent the massive damage and protect the passenger in the event of a crash.
Crashworthiness parameters for various thin-walled tubes made from metal or fibre/resin composites in different geometries have been studied.
A critical difference of tubular composites failure modes compared with metallic is the brittle collapse.
In addition, in composites, tubular failure modes are involved with micro-cracking development, delamination, fibre breakage, etc., instead of plastic deformation.
Implementation of composite materials in the field of crashworthiness is attributed to Hull, who in 80s and 90s of the last century studied extensively the crushing behaviour of fibre reinforced composite material.
He found that the composite materials absorbed high energy in the face of the fracture surface energy mechanism rather than plastic deformation as observed for metals [1].
This observation has inspired others to further investigation about crashworthiness characteristics of composite materials.
Studies have examined the axial crushing behaviour of fibre-reinforced tubes [2], fibreglass tubes [3,4], PVC tubes [5] and carbon fibre reinforced plastic (CFRP) tubes [6].
We have developed the theory of electrons carrying quantized orbital angular momentum.
To make connection to realistic situations, we considered a plane wave moving along the optic axis of a lens system, intercepted by a round, centered aperture.88In the experiment, this aperture carries the holographic mask.
It turns out that the movement along the optic axis can be separated off; the reduced Schrödinger equation operating in the plane of the aperture can be mapped onto Bessel's differential equation.
The ensuing eigenfunctions fall into families with discrete orbital angular momentum ℏm along the optic axis where m is a magnetic quantum number.
Those vortices can be produced by matching a plane wave after passage through a holographic mask with a fork dislocation to the eigenfunctions of the cylindrical problem.
Vortices can be focussed by magnetic lenses into volcano-like charge distributions with very narrow angular divergence, resembling loop currents in the diffraction plane.
Inclusion of spherical aberration changes the ringlike shape but does not destroy the central zero intensity of vortices with m≠0.
Partial coherence of the incident wave leads to a rise of the central intensity minimum.
It is shown that a very small source angle (i.e.
a very high coherence) is necessary so as to keep the volcano structure intact.
Their small angular width in the far field may allow the creation of nm-sized or smaller electron vortices but the demand for extremely high coherence of the source poses a serious difficulty.
We consider cosmological consequences of a conformal-invariant formulation of Einstein's General Relativity where instead of the scale factor of the spatial metrics in the action functional a massless scalar (dilaton) field occurs which scales all masses including the Planck mass.
Instead of the expansion of the universe we obtain the Hoyle–Narlikar type of mass evolution, where the temperature history of the universe is replaced by the mass history.
We show that this conformal-invariant cosmological model gives a satisfactory description of the new supernova Ia data for the effective magnitude–redshift relation without a cosmological constant and make a prediction for the high-redshift behavior which deviates from that of standard cosmology for z>1.7.
FabHemeLB is a Python tool which helps automate the construction and management of ensemble simulation workflows.
FabHemeLB is an extended version of FabSim [27] configured to handle HemeLB operations.
Both FabSim and FabHemeLB help to automate application deployment, execution and data analysis on remote resources.
FabHemeLB can be used to compile and build HemeLB on any remote resource, to reuse machine-specific configurations, and to organize and curate simulation data.
It can also submit HemeLB jobs to a remote resource specifying the number of cores and the wall clock time limit for completing a simulation.
The tool is also able to monitor the queue status on remote resources, fetch results of completed jobs, and can conveniently combine functionalities into single one-line commands.
In general, the FabHemeLB commands have the following structure:
This paper proposes a sentence stress feedback system in which sentence stress prediction, detection, and feedback provision models are combined.
This system provides non-native learners with feedback on sentence stress errors so that they can improve their English rhythm and fluency in a self-study setting.
The sentence stress feedback system was devised to predict and detect the sentence stress of any practice sentence.
The accuracy of the prediction and detection models was 96.6% and 84.1%, respectively.
The stress feedback provision model offers positive or negative stress feedback for each spoken word by comparing the probability of the predicted stress pattern with that of the detected stress pattern.
In an experiment that evaluated the educational effect of the proposed system incorporated in our CALL system, significant improvements in accentedness and rhythm were seen with the students who trained with our system but not with those in the control group.
Brodsky and Lepage [8] have proposed a formula for meson pair production which looks similar to (25), except for a different charge factor and the appearance of the timelike electromagnetic meson form factor instead of the annihilation form factor R(s).
This formula was obtained from the leading-twist result by neglecting part of the amplitudes with opposite photon helicities.
As has been pointed out in [9], this part is however not approximately independent of the pion distribution amplitude and not generically small.
We also remark that the appearance of Fπ(s) in the γγ→π+π− amplitude is no longer observed if corrections from partonic transverse momentum in the hard scattering process are taken into account, and that these corrections are not numerically small for the values of s we are dealing with [13].
Notice further that two-photon annihilation produces two pions in a C-even state, whereas the electromagnetic form factor projects on the C-odd state of a pion pair.
In contrast, our annihilation form factor R2π(s) is C-even as discussed after (24).
Finally, due to a particular charge factor, the Brodsky–Lepage formula leads to a vanishing cross section for γγ annihilation into pairs of neutral pseudoscalars.
The Substrate Induced Coagulation (SIC) coating process provides a self assembled and almost binder free coating with small particles.
Most research so far has been used to coat a variety of surfaces with highly conductive carbon blacks [34,35,36].
Layers deposited by this technique have been used in electromagnetic wave shielding, in the metallization process of through-holes in printed wiring boards, and in the manufacture of conducting polymers (such as Teflon) [37,38,39].
An advantage of this dip-coating process is that it can be used for any kind of surface, provided the substrate is stable in water and that the particles used for the coating form a meta-stable dispersion.
Recently, a non-aqueous SIC coating process of carbon black was developed by investigating the stabilities of non-aqueous dispersions [36].
These dispersions were used to prepare LiCoO2-composite electrodes for Li-ion batteries with an improved conductivity while keeping the content of active battery material high [35].
Typical physically-based 2D flood models solve the Shallow Water Equations (SWEs), requiring high computational resources.
Many of these models have been developed to obtain better performance, while maintaining the required accuracy, by reducing the complexity of the SWEs.
This reduction is usually achieved by approximating or neglecting less significant terms of the equations (Hunter et al., 2007; Yen and Tsai, 2001).
The JFLOW model (Bradbrook et al., 2004), Urban Inundation Model (UIM) (Chen et al., 2007), and the diffusive version of LISFLOOD-FP (Hunter et al., 2005) solve the 2D diffusion wave equations that neglect the inertial (local acceleration) and advection (convective acceleration) terms (Yen and Tsai, 2001).
The inertial version of LISFLOOD-FP (Bates et al., 2010) solves the SWEs without the advection term.
In either version of LISFLOOD-FP the flow is decoupled in the Cartesian directions.
Other models use the full SWEs but focus on the use of multi resolution grids or irregular mesh, like InfoWorks ICM (Innovyze, 2012) and MIKE FLOOD (DHI Software, 2014; Hénonin et al., 2013).
These last two models are commercial packages, and the code applied in the optimisation techniques is not in the public domain.
The lateral force, Q, is measured and recorded throughout the entire test by a piezoelectric load cell which is connected to the quasi-stationary LSMB.
The LSMB is mounted on flexures which provide flexibility in the horizontal direction so that the majority of the lateral force is transmitted though the much stiffer load path which contains the load cell as shown in Fig.
2.
Both displacement and load sensors have been calibrated (both externally and in-situ) in static conditions.
The load and displacement signals are sampled at a rate of two hundred measurements per fretting cycle at all fretting frequencies, with these data being used to generate fretting loops.
The loops were used to derive the contact slip amplitude and the energy coefficient of friction in each cycle according to the method suggested by Fouvry et al.
[17].
Average values for these were calculated for each test (the average coefficient of friction included values associated with the initial transients in the tests as suggested by Hirsch and Neu [18]).
Prior to assembling the miniature ADR, the mKCC MR heat switch could not be fully thermally characterised due to cryostat constraints.
However, based on experiments and research conducted at MSSL on a range of tungsten heat switches, the thermal conductivity has been estimated.
In Hills et al.
[8], an equation is derived which allows the thermal conductivity (κ) below 6K to be calculated as a function of magnetic field (B) and temperature (T) (see Eq.
(1)).
To estimate the performance of the mKCC heat switch, the parameters in Eq.
(1) have been taken from the measured thermal conductivity of another MSSL heat switch with the same 1.5mm square cross section, a free path length of 43cm and a RRR of 20,000; it has been observed from experiments conducted at MSSL that there is little change in the thermal performance for tungsten heat switches with a RRR between 20,000 and 32,000 (subject of a future publication) and therefore the performance of the 20,000 RRR heat switch has been assumed to be a good approximation.
Fig.
5 gives the calculated thermal conductivity of the mKCC switch at 0, 1, 2 and 3T based on Eq.
(1), where the constants b0, a1, a2, a3, a4 and n have the values 0.0328, 1.19×10−4, 3.57×10−6, 1.36, 0.000968 and 1.7 respectively.
It should be noted that the calculated thermal conductivity of the mKCC switch presented in Fig.
5 has been validated by comparing the experimental results of the miniature ADR with modelled predictions (this is discussed in Section 3.3).
(1)κ(T)=b0T2+1a1+a2T2T+Bna3T+a4T4
While virtualization technologies certainly reduce the complexity of using a system, and especially when working across multiple heterogeneous computing environments, they are not widely deployed in high performance computing scenarios.
As its name suggest, HPC seeks to obtain maximum performance from computing platforms.
Extra software layers impact detrimentally on performance, meaning that in HPC scenarios users typically run the applications as close to the ‘bare metal’ as possible.
In addition to the performance degradation introduced by virtualization technologies, choosing what details to abstract in a virtualized interface is itself very important.
Grid and cloud computing support different interaction models.
In grid computing, the user interacts with an individual resource (or sometimes a broker) in order to launch jobs into a queuing system.
In cloud computing, users interact with a virtual server, in effect putting them in control of their own complete operating system.
Both of these interaction models put the onus on the user to understand very specific details of the system that they are dealing with, making life difficult for the end user, typically a scientist who wants to progress his or her scientific investigations without any specific usability hurdles obstructing the pathway.
Functionally Graded Materials (FGMs), described in detail by Suresh and Mortensen [1], are a type of heterogeneous composite materials exhibiting gradual variation in volume fraction of their constituents from one surface of the material to the other, resulting in properties which vary continuously across the material.
The idea of a Functionally Graded Material is not a new one, there are in fact many natural materials which exhibit this property.
Study of bone, shell, balsawood and bamboo shows that they are all graded with their greatest strength on the outside, in areas where the greatest protection is required.
However it was not until the 1980s in Japan [2] that the idea of a Functionally Graded Material was actively researched in order to gain advances in heat resistant materials for use in aerospace and nuclear fission reactors.
The mesoporous silica particles were prepared by the surfactant self-assembly method described previously [18,24].
Briefly, a homogeneous solution of the soluble silica precursor, tetraethylorthosilicate (TEOS; Sigma-Aldrich Corp., St. Louis, MO), and hydrochloric acid was mixed in ethanol and water.
A surfactant, cetyltrimethylammonium bromide (CTAB; Sigma-Aldrich Corp., St. Louis, MO), with an initial concentration much less than the critical micelle concentration was added to lower the surface tension of the liquid mixture and act as the mesoporous structure-directing template.
Aerosol solutions of soluble silica plus surfactant were then generated with nitrogen as a carrier atomizing gas using a commercially available atomizer (Model 9392A, TSI, Inc., St. Paul, MN).
The aerosol droplets were solidified in a tube furnace at 400°C until dry.
Once dried, a durapore membrane filter, kept at 80°C, was used to collect the particles.
As a final step, the surfactant was removed at 400°C for 5h via calcination.
The surface of the mesoporous silica core in these studies was chemically modified with 10wt.% or 15wt.% by aminopropyltriethoxysilane (APTES; Sigma-Aldrich Corp., St. Louis, MO) conducted identically as previously described [17] to create a positive surface charge to increase loading efficiency of negatively charged cargo.
Further, Liu and colleagues report the colloidal stability of these protocells with lipid bilayers, excess amount of liposomes (50μg liposomes per 0.5mg silica were used [18]).
SPS has been utilized in several studies to retain the nanostructure of aluminum alloy powders during consolidation.
Ye et al.
investigated the effect of processing of cryomilled Al 5083 powder via SPS [13].
X-ray Diffraction (XRD) grain size calculations before and after SPS showed that the average grain size of the alloy only increased from 25nm to 50nm (from powder to bulk state).
Subsequently, the hardness values obtained through nanoindentation for specimens of AA5083 produced via SPS were highly improved in comparison to conventional sintering methods were grain coarsening takes place on a larger scale.
In another study the combination of cryomilling and SPS of AA-5356/B4C nanocomposites powder was found to largely improve the microhardness and flexural strengths of the bulk nanocomposite.
Rana et al.
[14] investigated the effect of SPS on mechanically milled AA6061 (Al–Mg–Si) micro-alloy powder.
The average grain size after 20h of milling was ∼35nm and increased to only ∼85nm after processing with SPS at 500°C.
Microhardness and compressive tests were carried out on the consolidated near full density specimens of both unmilled and milled powders and the results showed significant increase in both hardness and compressive strengths for the milled nanocrystalline powders as a result of the very fine grain size.
In this paper, crystal plasticity model, in combination with XFEM, has been applied to study cyclic deformation and fatigue crack growth in a nickel-based superalloy LSHR (Low Solvus High Refractory) at high temperature.
The first objective of this research was to develop and evaluate a RVE-based finite element model with the incorporation of a realistic material microstructure.
The second objective of this work was to determine the parameters of a crystal plasticity constitutive model to describe the cyclic deformation behaviour of the material by using a user-defined material subroutine (UMAT) interfaced with the finite element package ABAQUS.
The model parameters were calibrated from extensive finite element analyses to fit the monotonic, stress relaxation and cyclic test data.
The third objective was to predict crack growth by combining the XFEM technique and the calibrated crystal plasticity UMAT, for which accumulated plastic strain was used as the fracture criterion.
The first-principles calculations are performed using the Cambridge Serial Total Energy Package (CASTEP) [21] which implements the plane-wave pseudopotential DFT method.
The exchange correlation functional is approximated using the generalized gradient approximation (PBE-GGA) [22], and the electron–ion interactions are described by Vanderbilt-type ultrasoft pseudopotentials [23].
The plane wave basis set is truncated at a cutoff of 400eV, and the Brillouin-zone sampling was performed using the Monkhorst-Pack scheme with a k-point spacing in reciprocal space of 0.04Å−1.
Tests show that these computational parameters give results that are sufficiently accurate for present purposes.
The ferromagnetism of nickel is accounted for by performing all calculations using spin polarization, starting at a ferromagnetic initial configuration and relaxing towards its ground state.
However, for all compositions considered, the ground state electronic structure of each alloy is found to exhibit only very weak ferromagnetism, and the effect is not thought to influence their phase stability.
Table 1 shows the calculated equilibrium lattice constants of the η phase at various Ti concentrations, using partially ordered ηP structures.
The change in lattice constant upon Ti alloying is relatively small, but can be related to the ∼10% larger covalent radius of Ti.
The calculated lattice constants are in good agreement with the experimental values, which relate to an alloy with a Al/Ti ratio of ∼2.75.
One major goal of current nuclear physics is the observation of at least partial restoration of chiral symmetry.
Since the chiral order parameter 〈q̄q〉 is expected to decrease by about 30% already at normal nuclear matter density [1–4], any in-medium change due to the dropping quark condensate should in principle be observable in photonuclear reactions.
The conjecture that such a partial restoration of chiral symmetry causes a softening and narrowing of the σ meson as the chiral partner of the pion in the nuclear medium [5,6] has led to the idea of measuring the π0π0 invariant mass distribution near the 2π threshold in photon induced reactions on nuclei [7].
In contrast to its questionable nature as a proper quasiparticle in vacuum, the σ meson might develop a much narrower peak at finite baryon density due to phase-space suppression for the σ→ππ decay, hence making it possible to explore its properties when embedded in a nuclear many-body system [8–11].
Measuring a threshold enhancement of the π0π0 invariant mass spectrum might serve as a signal for the partial restoration of chiral symmetry inside nuclei and, therefore, give information about one of the most fundamental features of QCD.
DPD was first proposed in order to recover the properties of isotropy (and Galilean invariance) that were broken in the so-called lattice-gas automata method [5].
In DPD, each body is regarded as a coarse-grained particle.
These particles interact in a soft (and short-ranged) potential, allowing larger integration timesteps than would be possible in MD, while simultaneously decreasing the number of degrees of freedom required.
As in Langevin dynamics, a thermostat consisting of well-balanced damping and stochastic terms is applied to each particle.
However, unlike in Langevin dynamics, both terms are pairwise and the damping term is based on relative velocities, leading to the conservation of both the angular momentum and the linear momentum.
The property of Galilean invariance (i.e., the dependence on the relative velocity) makes DPD a profile-unbiased thermostat (PUT) [6,7] by construction and thus it is an ideal thermostat for nonequilibrium molecular dynamics (NEMD) [8].
The momentum is expected to propagate locally (while global momentum is conserved) and thus the correct hydrodynamics is expected in DPD [8], as demonstrated previously in [9].
Due to the aforementioned properties, DPD has been widely used to recover thermodynamic, dynamical, and rheological properties of complex fluids, with applications in polymer solutions [10], colloidal suspensions [11], multiphase flows [12], and biological systems [13].
DPD has been compared with Langevin dynamics for out-of-equilibrium simulations of polymeric systems in [14], where as expected the correct dynamic fluctuations of the polymers were obtained with the former but not with the latter.
It is known that as the temperature of the sample rises, the Lorentz mechanism remains dominant until Tc of steel is reached (770°C for a low carbon steel), when the magnetostrictive mechanism becomes more efficient [15].
Previously this has been thought due to a thin ferromagnetic oxide layer on the sample surface, the surface being cooler than the bulk of the material [16,17].
This layer concentrates the magnetic field, increasing generation efficiency.
Recent studies also show that rearrangement of the magnetic moments from ordered domains to a disordered state at a magnetic phase transition lowers the magnetostrictive constant.
This ferromagnetic to paramagnetic transition is accompanied by large changes in the efficiency of electromagnetic ultrasound generation leading to the use of EMATs as a method of studying phase transitions in magnetic alloys [18].
Inequality (22) indicates that the maximum-norm is the loosest among all p-norms.
Fortunately, this loosest constraint would not seriously affect the accuracy since the value of ||y||∞ is comparable to that of the 2-norm and 1-norm.
The maximum-norm provides us with the largest number of possible solutions under a given error limitation [24].
This would greatly enhance the possibility of finding a group of optimized coefficients when scanning a vast solution set.
On the other hand, checking the maximum deviation sounds more reasonable than checking the “distance” between the accurate and approximated wave numbers since it is not working in the space domain.
Therefore, we chose the maximum-norm as our criterion for designing the objective functions to extend the accurate wave number coverage as widely as possible.
It is interesting to quantify the effects of the Schmidt number and the chemical reaction rate on the bulk-mean concentration of B in water.
The data could present important information on evaluating the environmental impacts of the degradation product of B, as well as acidification of water by the chemical reaction.
Here, the bulk-mean concentration of B is defined by(24)CB⁎¯=∫01〈CB⁎〉(z⁎)dz⁎ Fig.
15 depicts the effect of the Schmidt and the chemical reaction rate on the bulk-mean concentration CB⁎¯.
It is worth to mention here that the bulk-mean concentration of B reaches approximately 0.6 as the chemical reaction rate and the Schmidt number increase to infinite, and the concentration is smaller than the equilibrium concentration of A at the interface.
This figure indicates that progress of the chemical reaction is somewhat interfered by turbulent mixing in water, and the efficiency of the chemical reaction is up to approximately 60%.
The efficiency of the chemical reaction in water will be a function of the Reynolds number of the water flow, and the efficiency could increase as the Reynolds number increases.
We need an extensive investigation on the efficiency of the aquarium chemical reaction in the near future to extend the results of this study further to establish practical modelling for the gas exchange between air and water.
Fig.
9 displays the growth of two of the main corrosion products that develop or form on the surface of Cu40Zn with time, hydrozincite (Fig.
9a) and Cu2O (Fig.
9b).
It should be remembered that both phases were present already from start of the exposure.
The data is presented in absorbance units and allows comparisons to be made of the amounts of each species between the two Cu40Zn surfaces investigated, DP and HZ7.
The tendency is very clear that the formation rates of both hydrozincite and cuprite are quite suppressed for Cu40Zn with preformed hydrozincite (HZ7) compared to the diamond polished surface (DP).
In summary, without being able to consider the formation of simonkolleite, it can be concluded that an increased surface coverage of hydrozincite reduces the initial spreading ability of the NaCl-containing droplets and thereby lowers the overall formation rate of hydrozincite and cuprite.
Moreover, one observes segregation effects by the XRD analysis, which probably took place at high temperature, and were partially quenched to room temperature.
The phase analysis showed up to three distinct phases, which should have hence a distinct measurable phase transition temperature, if they crystallise from the liquid on the surface.
In the thermograms these effects are not observable as different solidification arrest or clear inflections.
The proportion of new appearing phases is small and therefore the latent heat released by this new phase will be also small.
The reflected light signal technique only showed one phase change during cooling.
As well, the location of this segregation cannot be determined exactly in the molten pool or later in the re-solidified material.
At the surface, where the temperature is measured, the material analysis by Raman spectroscopy has not shown signs of segregation, so that also the uncertainties in composition for the phase transition are taken from the uncertainties from the XRD analysis for the most abundant phase at each composition in re-solidified material.
The viscoelastic behavior of elastomers containing small amounts of unattached chains has been investigated to characterize the dynamics of the polymer chains trapped in fixed networks [66–68].
Polymer chains trapped in fixed networks constitute a simpler system for the study of the polymer chain dynamics than the corresponding uncrosslinked polymer melts.
This is because the complicated effect of the motion of the surrounding chains on the dynamics of the probe chain – called “constraint release” [69] – is absent in the fixed network systems.
Most of the earlier studies employed randomly crosslinked elastomers as host networks.
In this case, precise control of the mesh size of the host networks is not possible, and the mesh size has a broad distribution.
The end-linking systems give the host networks a more uniform mesh size, and they can control the mesh size by the size of the precursor chains.
We investigated the dynamic viscoelasticity of end-linked PDMS elastomers containing unattached linear PDMS as functions of the size of the unattached chains (Mg) and the network mesh (Mx) (Fig.
9a) [70].
We employed two types of host networks with Mx>Me and Mx<Me where Me (≈10,000 for PDMS) is the molecular mass between adjacent entanglements in the molten state.
The Mx>Me and Mx<Me networks (designated as NL and NS, respectively) were designed by end-linking the long (Mn=84,000) and short precursor chains (Mn=4,550), respectively.
The mesh of the NL networks is dominated by trapped entanglements, while that of the NS network is governed by chemical cross-links.
The most widely used ion source in FIB instruments is a gallium (Ga) liquid metal ion source (LMIS) [1].
Gallium is attractive as an ion source because of its low melting temperature (29.8°C at standard atmospheric pressure [4]) and its low volatility [1].
However, some materials show sensitivity to the Ga ion beam.
This sensitivity is manifested as changes in the structure and chemical composition of the starting material upon exposure to the Ga ion beam [5].
Group III–V compound semiconductors are one class of materials that show such sensitivity.
Cryo-FIB milling has recently been reported to suppress the reactions between the Ga ion beam and III–V materials [6].
The suggested advantage of cryo-FIB milling over room temperature milling of Group III–V materials is appealing, given the variety of present and potential future applications for these materials (e.g., as electronic or photonic devices given the favorable electron transport and direct band gap properties associated with several III–V semiconductor systems).
A living polymerization is a reaction without transfer and termination reactions that can proceed up to complete monomer conversion.
In addition, when initiation is quantitative and fast compared to the propagation reaction, polymers with precisely controlled chain length and narrow molar mass distribution can be obtained.
In the case of an industrial styrene polymerization this would permit to avoid any specific washing or degassing steps, which are necessary in the radical process to remove residual monomer and low molar mass oligomers.
Since head-to-head defects along the chains are absent, anionic polystyrene would exhibit also a better thermal stability than radical one.
Therefore, production of anionic polystyrene (PS) would be of interest if the conditions required to control the polymerization could be adapted to the market and be able to compete economically with industrial radical processes.
The use of organic solvents and of expensive alkyllithium initiators, as well as the relatively low reaction temperatures required, was some important limitation to overcome.
The possibilities to achieve a quantitative living-like anionic polymerization of styrene in the absence of solvent and at elevated temperature, using inexpensive initiating systems, were the main targets identified to tremendously decrease the cost of the anionic process.
This implied at first to control the reactivity and stability of initiating and propagating active species in such unusual operating conditions.
This research traces the implementation of an information system in the form of ERP modules covering tenant and contract management in a Chinese service company.
Misalignments between the ERP system specification and user needs led to the adoption of informal processes within the organisation.
These processes are facilitated within an informal organisational structure and are based on human interactions undertaken within the formal organisation.
Rather than to attempt to suppress the emergence of the informal organisation the company decided to channel the energies of staff involved in informal processes towards organisational goals.
The company achieved this by harnessing the capabilities of what we term a hybrid ERP system, combining the functionality of a traditional (formal) ERP installation with the capabilities of Enterprise Social Software (ESS).
However the company recognised that the successful operation of the hybrid ERP system would require a number of changes in organisational design in areas such as reporting structures and communication channels.
A narrative provided by interviews with company personnel is thematised around the formal and informal characteristics of the organisation as defined in the literature.
This leads to a definition of the characteristics of the hybrid organisation and strategies for enabling a hybrid organisation, facilitated by a hybrid ERP system, which directs formal and informal behaviour towards organisational goals and provides a template for future hybrid implementations.
An early attempt to combine sets and networks in a single visualization relied on first drawing an Euler diagram then placing a graph inside it [30], however the sets were often visualized with convoluted, difficult to follow curves.
In addition, only limited kinds of set data could be shown as the system was limited to well-formed Euler diagrams.
Compound graphs can be used to represent restricted kinds of grouped network data [8].
Graph clusters are visualized with transparent hulls by Santamaria and Theron [39].
However, the technique removes edges from the graph and it is not sufficiently sophisticated for arbitrary overlapping sets.
Itoh et al.
[24] proposed to overlay pie-like glyphs over the nodes in a graph to encode multiple categories.
Each set is hence represented using disconnected regions that are linked by having the same colour.
This causes difficulties with tasks that involve finding relations between sets such as T1, T3 and T4 in Section 5.3.
A related class of techniques visualize grouping information over graphs using convex hulls, such as Vizster [22].
However, they do not support visualizing set overlaps.
The product change between batches #1/#2 and the others is the most influential on the test results.
The redesign and upgrade to 110-nm process technology reduces the pass rate at LNT by approximately half.
This is mainly caused by the increased incidence of erase and program timeouts with some contribution from long erase and program times and bit errors.
The difference in pass rates at 88K between batches #3/#4 and #5/#6, which use the same process technology with the same dimensions, can be explained by the fabrication in different assembly lines, where other processes or base materials may have been changed.
This means different tolerances in base materials and production process, which are more pronounced the lower the temperature.
Some of the differences of technology scale may reflect shifts in transistor parameters such as transconductance/gain, threshold voltage, and threshold slope [7].
The measurements presented here provide evidence for the existence of di-cluster structures in 10–12,14Be.
Certainly, if the breakup process samples the overlap between the wavefunctions of the ground state and the excited states, the first-chance cluster breakup cross-sections, shown in Fig.
4(a), indicate that the xHe+A−xHe cluster structure does not decrease over the mass range A=10, 12 and 14.
Given also that the decay energy threshold increases with mass number, the present data may even indicate a slight increase in clustering.
The breakup cross-sections also appear to demonstrate that these nuclei possess a stronger structural overlap with an α–Xn–α configuration, although the reaction mechanics by which this final state is reached may be complex.
That is to say that the dominant structural mode of the neutron rich isotopes may be identified with two alpha-particles plus valence neutrons.
These comprehensive measurements of the neutron-removal and cluster breakup for the first time provide experimental data whereby the structure of the most neutron-rich Be isotopes can be modeled via their reactions.
Production of charmonium states J/ψ and ψ′ in nucleus–nucleus collisions has been studied at CERN SPS over the previous 15 years by the NA38 and NA50 Collaborations.
This experimental program was mainly motivated by the suggestion [1] to use the J/ψ as a probe of the state of matter created at the early stage of the collision.
The original picture [1] (see also [2] for a modern review) assumes that charmonia are created exclusively at the initial stage of the reaction in primary nucleon–nucleon collisions.
During the subsequent evolution of the system, the number of hidden charm mesons is reduced because of: (a) absorption of pre-resonance charmonium states by nuclear nucleons (normal nuclear suppression), (b) interactions of charmonia with secondary hadrons (comovers), (c) dissociation of cc̄ bound states in deconfined medium (anomalous suppression).
It was found [3] that J/ψ suppression with respect to Drell–Yan muon pairs measured in proton–nucleus and nucleus–nucleus collisions with light projectiles can be explained by the so-called “normal” (due to sweeping nucleons) nuclear suppression alone.
In contrast, the NA50 experiment with a heavy projectile and target (Pb+Pb) revealed essentially stronger J/ψ suppression for central collisions [4–7].
This anomalous J/ψ suppression was attributed to formation of quark–gluon plasma (QGP) [7], but a comover scenario cannot be excluded [8].
ObjectiveElectrically evoked auditory steady-state responses (EASSRs) are neural potentials measured in the electroencephalogram (EEG) in response to periodic pulse trains presented, for example, through a cochlear implant (CI).
EASSRs could potentially be used for objective CI fitting.
However, EEG signals are contaminated with electrical CI artifacts.
In this paper, we characterized the CI artifacts for monopolar mode stimulation and evaluated at which pulse rate, linear interpolation over the signal part contaminated with CI artifact is successful.MethodsCI artifacts were characterized by means of their amplitude growth functions and duration.ResultsCI artifact durations were between 0.7 and 1.7ms, at contralateral recording electrodes.
At ipsilateral recording electrodes, CI artifact durations are range from 0.7 to larger than 2ms.ConclusionAt contralateral recording electrodes, the artifact was shorter than the interpulse interval across subjects for 500pps, which was not always the case for 900pps.SignificanceCI artifact-free EASSRs are crucial for reliable CI fitting and neuroscience research.
The CI artifact has been characterized and linear interpolation allows to remove it at contralateral recording electrodes for stimulation at 500pps.
In this work we develop a new approach to DEA suitable for modelling three-dimensional problems.
The present DEA methods rely on the fact that one can easily parametrise the boundary of the region being modelled, and then apply an orthonormal basis approximation over the resulting boundary phase space coordinate system.
In two dimensions this is simple as the boundary may be parametrised along its arc-length and the associated momentum (or direction) coordinate taken tangential to the boundary.
The basis can be any suitable (scaled) univariate basis in both position and momentum, such as a Fourier basis [8] or Chebyshev polynomials [9].
Defining a suitable parametrisation for the spatial coordinate in three-dimensions becomes much more difficult.
In momentum space spherical polar coordinates may be employed and so these problems do not arise.
Apache Pig is a platform for creating MapReduce workflows with Hadoop.
These workflows are expressed as directed acyclic graphs (DAGs) of tasks that exist at a conceptually higher level than their implementations as series of MapReduce jobs.
Pig Latin is the procedural language used for building these workflows, providing syntax similar to the declarative SQL commonly used for relational database systems.
In addition to standard SQL operations, Pig can be extended with user-defined functions (UDFs) commonly written in Java.
We adopted Pig for our implementation of the correlator to speed up development time, allow for ad hoc workflow changes, and to embrace the Hadoop community׳s migration away from MapReduce towards more generalized DAG processing (Mayer, 2013).
Specifically, in the event that future versions of Hadoop are optimized to support paradigms other than MapReduce, Pig scripts could take advantage of these advances without recoding, whereas explicit Java MapReduce jobs would need to be rewritten.
In the bag model and in linear or harmonic oscillator confining potentials, the first excited S-state lies above the lowest P-state, making the predicted Roper mass heavier than the lightest negative parity baryon mass.
Pairwise spin-dependent interactions must reverse the level ordering.
As mentioned earlier, color-spin interactions fail in this regard [29], while flavor-spin interactions produce the desired effect.
Since the q3 color wave function is antisymmetric, the flavor-spin-orbital wave function is totally symmetric.
For all quarks in an S-state, the flavor-spin wave function is totally symmetric all by itself and leads to the most attractive flavor-spin interaction.
If one quark is in a P-state, the orbital wave function is mixed symmetry and so is the flavor-spin wave function, and the flavor-spin interaction is a less attractive.
In the SU(3)F symmetric case, Eq.
(1), one obtains mass splittings (2)ΔMχ=−14Cχ,N(939),N∗(1440),−4Cχ,Δ(1232),−2Cχ,N∗(1535).
Here we have approximated the N∗(1535) as a state with total quark spin-1/2.
AA 2024-T3 aluminium alloy is widely used for aerospace applications due to its high strength to weight ratio and high damage tolerance that result from copper and magnesium as the principal alloying elements and appropriate thermomechanical processing.
The microstructure of the alloy is relatively complex and a number of compositionally-distinct phases have been identified [1].
Although possessing favourable mechanical properties, the alloy is relatively susceptible to corrosion and generally requires surface treatment in practical applications.
The corrosion behaviour of the alloy is particularly affected by the presence of the intermetallic particles due to their differing potentials with respect to the alloy matrix [2–9].
Copper-containing second phase particles at the alloy surface are particularly detrimental to the corrosion resistance as they provide preferential cathodic sites [2,10].
One of the principle types of second phase particle that is important to the corrosion behaviour of the alloy is the S phase (Al2CuMg) particle [1,11].
Dealloying of S phase particles, which may account for ∼60% of the constituent particles in AA2024 alloys [11], is commonly observed when the alloy is exposed to an aggressive environment.
The particles are considered as important initiation sites for severe localized corrosion in the alloy [11–22].
The dealloying of the S phase particles and the resulting enrichment of copper result in a decrease of the Volta potential with respect to the matrix and hence the dealloyed particles become active cathodic sites [23–25].
Traditionally, archaeologists have recorded sites and artefacts via a combination of ordinary still photographs, 2D line drawings and occasional cross-sections.
Given these constraints, the attractions of 3D models have been obvious for some time, with digital photogrammetry and laser scanners offering two well-known methods for data capture at close range (e.g.
Bates et al., 2010; Hess and Robson, 2010).
The highest specification laser scanners still boast better positional accuracy and greater true colour fidelity than SfM–MVS methods (James and Robson, 2012), but the latter produce very good quality models nonetheless and have many unique selling points.
Unlike traditional digital photogrammetry, little or no prior control of camera position is necessary, and unlike laser scanning, no major equipment costs or setup are involved.
However, the key attraction of SfM–MVS is that the required input can be taken by anyone with a digital camera and modest prior training about the required number and overlap of photographs.
A whole series of traditional bottlenecks are thereby removed from the recording process and large numbers of archaeological landscapes, sites or artefacts can now be captured rapidly, in the field, in the laboratory or in the museum.
Fig.
2a–c shows examples of terracotta warrior models for which the level of surface detail is considerable.
This paper has highlighted a band of frequencies, outside the conventional operation range, and close to electrical resonance of an eddy current probe, where the magnitude of impedance SNR reaches a peak.
The SNR of scans of three slots of varying depth were enhanced by a factor of up to 3.7, from the SNR measured at 1MHz.
This is a result of a defect-decoupling resonance-shift effect and is referred to as the near electrical resonance signal enhancement (NERSE) phenomenon.
NERSE frequency operation has significant potential for ECT inspection, and opens up a range of investigative possibilities.
Within this investigation, only the magnitude of the electrical impedance has been analyzed.
An immediate extension of this investigation will be to consider phase information, and determine whether a similar exploitable NERSE effect exists.
First results from RHIC on charged multiplicities, evolution of multiplicities with centrality, particle ratios and transverse momentum distributions in central and minimum bias collisions, are analyzed in a string model which includes hard collisions, collectivity in the initial state considered as string fusion, and rescattering of the produced secondaries.
Multiplicities and their evolution with centrality are successfully reproduced.
Transverse momentum distributions in the model show a larger pT-tail than experimental data, disagreement which grows with increasing centrality.
Discrepancies with particle ratios appear and are examined comparing with previous features of the model at SPS.
Copper-catalyzed Huisgen cycloadditions have been recently extensively studied by polymer chemists for the synthesis of functional polymers (either end-functional or side-functional).
The post-functionalization of synthetic polymers is an important feature of macromolecular engineering as many polymerization mechanisms are rather sensitive to the presence of bulky or functional groups.
For example, a wide variety of telechelic polymers (i.e.
polymers with defined chain-ends) can be efficiently prepared using a combination of atom transfer radical polymerization (ATRP) and CuAAC.
This strategy was independently reported in early 2005 by van Hest and Opsteen [31], Lutz et al.
[32], and Matyjaszewski et al.
[33].
Such step was important since ATRP is a very popular polymerization method in modern materials science [34,35].
Indeed, ATRP is a facile technique, which allows the preparation of well-defined polymers with narrow molecular weight distribution, predictable chain length, controlled microstructure, defined chain-ends and controlled architecture [36–41].
However, the range of possibilities of ATRP can be further broadened by CuAAC.
For instance, the ω-bromine chain-ends of polymers prepared by ATRP can be transformed into azides by nucleophilic substitution and subsequently reacted with functional alkynes (Scheme 3) [32].
Due to the very high chemoselectivity of CuAAC, this method is highly modular and may be used to synthesize a wide range of ω-functional polymers.
Moreover, the formed triazole rings are not “passive” spacers but interesting functions exhibiting H-bonds capability, aromaticity and rigidity.
Each hit position inside the drift chambers was calculated from the drift time digitized by a flash analog-to-digital converter.
The calculation was carried out based on a relation between the hit position and the drift time (x–t relation).
The x–t relation was precisely calculated by a drift chamber simulation package, GARFIELD [20], and a gas property simulation package, MAGBOLTZ [21].
Although the chambers were constructed carefully with a tolerance of 100 μm, there was a small position deviation of wires and field-shaping patterns, which could locally modify the electric field.
In order to take account of the limited accuracy in the chamber manufacturing, a correction was commonly applied to the calculated x–t relation throughout the experiments.
The correction was obtained to minimize the χ2 in the fitting of straight tracks of clean muon events observed on the ground without magnetic field.
The correction was as small as expected from the accuracy in the chamber manufacturing.
During the observations, the x–t relation was affected by the variation in the pressure and temperature of the chamber gas.
In order to take account of these time-dependent variations, the x–t relation was calibrated for each data-taking run.
Especially in calibrating the x–t relation of ODCs, an absolute reference positions were provided by SciFi, which are not affected by the variation in the pressure nor temperature.
The main objective of this manuscript is to present and discuss the application of SLAMM to the New York coast.
Although the base analysis considers a range of different possible SLR scenarios, the effects of various sources of uncertainties such as input parameters and driving data are not accounted for.
In addition, refined and site-specific data are often not available requiring the use of regional data collected from literature and professional judgement in order to run the model.
To ignore the effects of these uncertainties on predictions may make interpretation of the results and subsequent decision making misleading since the likelihood and probabilities of predicted outcomes would be unknown.
A unique capability of the current version of SLAMM is the ability to aggregate multiple types of input-data uncertainty to create outputs accompanied by probability statements and confidence intervals.
Uncertainty in elevation data layers have been considered by several modeling groups to various extents (Gesch, 2009; Gilmer and Ferdaña, 2012; Schmid et al., 2014).
However, to the best of our knowledge, no other marsh migration model simultaneously accounts for the combined effects of uncertainty in spatial inputs (DEM, VDATUM, etc.)
and parameter choices (accretion rates, tide ranges, etc.)
on landcover projections.
This added feature of SLAMM allows results to be evaluated in terms of their likelihood of occurrence with respect to input-data and parameter uncertainties.
Further, by assigning wide ranges of uncertainty when appropriate, it permits the production of meaningful projections in areas where high-quality local data are not available.
This figure demonstrates that changes in the measure of bitumen content create sizable differences in the stiffness modulus of asphaltic samples that include waste glass cullet.
As the percentage of glass increases, the measure of the stiffness modulus of modified asphalt increases too.
But with pass of optimum measure of glass the stiffness modulus of asphaltic samples decrease.
This trend in total of percentages of bitumen content is existing.
Due to that waste glass cullet has no suction; the trend does not extend to measuring the stiffness modulus of asphaltic samples including waste glass cullet with different percentage of bitumen content.
Glass particles do not absorb any bituminous material, so it is necessary to decrease the bitumen content with the addition of glass cullet.
According to Fig.
2 and the results of the Marshall tests, the optimum bitumen measures decrease significantly in samples that include higher percentages of waste glass cullet.
As the percentage of optimum bitumen content is 1% more in samples without waste glass cullet in comparison with saphaltic samples that include 20% waste glass cullet.
The stiffness modulus of asphaltic samples that include waste glass cullet increased due to additional interlocking between the aggregate and the angularity of particles of glass cullet content.
The increase in the intrusive friction angle because of the glass particles’ increased angularity is the main reason for the addition of the stiffness modulus of asphaltic samples that include waste glass cullet.
But as the percentage of glass content reaches greater than 15%, the particles’ abundance cause slip these particles on together.
The stiffness modulus of samples decreases as the percentage of glass cullet increases.
The variations in the stiffness modulus of asphaltic samples that include different percentages of waste glass cullet at different temperature are shown in Fig.
3.
Some methods use 1D radial profiles obtained from circular averaging of 2D experimental PSD [4,8,11] or by elliptical averaging [17].
An inadequacy of circular averaging is that it neglects astigmatism.
Astigmatism distorts the circular shape of the Thon rings and thus decreases their modulation depth in the obtained 1D profile.
A few algorithms that consider astigmatism involve concepts such as dividing the PSD into sectors where Thon rings are approximated by circular arcs [15,21], applying Canny edge detection to find the rings [17] prior to elliptical averaging, determining the relationship between the 1D circular averages with and without astigmatism [22], or using a brute-force scan of a database containing precalculated patterns as in ATLAS [23].
Some other approaches for estimating CTF parameters do a fully 2D PSD optimization [12,14,18,20] but they usually regulate and fit numerous parameters by an extensive search that does not guarantee convergence.
Furthermore, only a few schemes that were developed for defocus estimation provide an error analysis [23,24].
Ultrasound (US) can initiate the release of drugs from liposomes via an event called inertial cavitation, whereby the rarefactional phase of an ultrasound wave causes the expansion of a gas bubble followed by a violent collapse due to the inertia of the surrounding media.
This collapse creates shock waves which can disrupt the stability of co-localised liposomal drug carriers.
To date, studies have concentrated on the use of low frequency or high intensity US to generate gas bubbles in situ, and most recently such parameters have been used to achieve a variable level of triggered drug release following an intratumoral injection of liposomes [14].
However, concerns persist over the damage to non-target tissue that such US exposure parameters may cause and whether ultimately they will be widely clinically applicable.
An alternative strategy is to utilise high-frequency US pulses at pressures in the diagnostic range in the presence of pre-existing gas bubbles.
This provides an inertial cavitation stimulus for drug release using safe, clinically achievable US exposure conditions and approved US contrast agents [15].
Indeed, in the context of improving the delivery of therapeutics such as oncolytic viruses, this approach has already shown great promise [16].
A further advantage of this approach is that US-induced cavitation events produce distinct acoustic emissions that can be recorded and characterised providing non-invasive feedback, a feature which has proven useful in ablative US applications [17–19].
As future work on the protocol, we would promote two items.
Firstly, the two mobility models that we have considered in this work propose possible way to capture social context in the way nodes move in the physical space, yet still potentially allowing nodes to explore the geographical regions considered in its entirety.
Further insights to the performance potential could be given through the assessment of the protocol with other mobilities that can extend the physical region of movement as well as impose potential restrictions on the nodes mobility, for example by forcing similar nodes to move within specifically defined areas.
Secondly, the different forwarding modes introduced in Section  3.3 express different levels of cooperation across the network.
The push-community mode, for example, is a form of interest-community selfishness and assumes reciprocation in the nodes’ behaviour.
The vulnerability (resp.
resilience) of the protocol to different instances of node misbehaviours is a research item worth exploring.
In this section we wish to calculate the cross section for the absorption of massless scalars by the self-dual string in the world volume of the M-theory five-brane.
We will adopt an entirely world volume approach similar to that of [21–23].
We begin by writing the equation satisfied by the s-wave with energy ω, φ(r,t)=φ(r)eiωt, of the linear fluctuations of the four overall transverse scalars about the self-dual string, (it is known that there are problems when one considers higher angular momentum modes [23], one must take care with the validity of the linearized approximation, this is discussed in [13]): (15)ρ−3ddρρ3ddρ+1+R6ω6ρ6φ(ρ)=0, where ρ=rω, R=Q1/3ℓp.
Note, as pointed out by [11] world volume solitons have a much sharper potential than the Coulomb type potential typical of brane solutions in supergravity; thus this scattering is different to that of the string in six-dimensional supergravity.
Nevertheless, for small ωR one may solve this problem by matching an approximate solution in the inner region to an approximate solution in the outer region; this follows closely the supergravity calculation [24].
In representing wetland-river interactions involving GIWs, many models assume that the wetland can discharge into a river but cannot receive overbank flows from it.
In such models, the volume of water (or water level elevation) in a wetland and its corresponding threshold value (predominantly controlled by outlet elevation) are the prime determinants of wetland outflow (Feng et al., 2012; Hammer and Kadlec, 1986; Johnson et al., 2010; Kadlec and Wallace, 2009; Powell et al., 2008; Voldseth et al., 2007; Wen et al., 2013; Zhang and Mitsch, 2005).
However, in regions characterised by widespread riparian wetlands that are hydraulically connected with adjacent rivers, wetland-river interaction is likely to be bidirectional.
Such interactions should be quantified according to hydraulic principles involving relative river and wetland water level elevations as well as the properties of the connection between the two (Kouwen, 2013; Liu et al., 2008; Min et al., 2010; Nyarko, 2007; Restrepo et al., 1998).
In the WATFLOOD model, for instance, riparian wetland-river interaction is modelled using the principle of Dupuit-Forchheimer lateral/radial groundwater flow (Kouwen, 2013).
Since exchange between riparian wetlands and rivers can occur over the surface and/or through the subsurface, Restrepo et al.
(1998) incorporated an equivalent transmissivity expression, obtained for wetland vegetation and the subsurface soil, into the Darcy flow equation of the MODFLOW model.
